flume  kafka  hdfs hive spark

1.flume
flume　agent是一个jvm进程，

核心：　　传送数据：　监控目录或者端口　发送到　hdfs或者kafka

拦截器功能，配置　过滤数据．

spooldir　适合不同文件新增，传输完成　添加.complated后缀

taildir 适合单个文件实时更新，比如ｌｏｇ实时


source  channel  sink

Exec Source
Example for agent named a1:

a1.sources = r1
a1.channels = c1
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /var/log/secure
a1.sources.r1.channels = c1

a1.sources.tailsource-1.type = exec
a1.sources.tailsource-1.shell = /bin/bash -c
a1.sources.tailsource-1.command = for i in /path/*.txt; do cat $i; done


Spooling Directory Source
Example for an agent named agent-1:

a1.channels = ch-1
a1.sources = src-1

a1.sources.src-1.type = spooldir
a1.sources.src-1.channels = ch-1
a1.sources.src-1.spoolDir = /var/log/apache/flumeSpool
a1.sources.src-1.fileHeader = true



1.1Taildir Source　监控目录多目录，多文件

Example for agent named a1:

a1.sources = r1
a1.channels = c1
a1.sources.r1.type = TAILDIR
a1.sources.r1.channels = c1
a1.sources.r1.positionFile = /var/log/flume/taildir_position.json
a1.sources.r1.filegroups = f1 f2
a1.sources.r1.filegroups.f1 = /var/log/test1/example.log
a1.sources.r1.headers.f1.headerKey1 = value1
a1.sources.r1.filegroups.f2 = /var/log/test2/.*log.*
a1.sources.r1.headers.f2.headerKey1 = value2
a1.sources.r1.headers.f2.headerKey2 = value2-2
a1.sources.r1.fileHeader = true
a1.sources.ri.maxBatchCount = 1000

Kafka Source
Example for topic subscription by comma-separated topic list.

tier1.sources.source1.type = org.apache.flume.source.kafka.KafkaSource
tier1.sources.source1.channels = channel1
tier1.sources.source1.batchSize = 5000
tier1.sources.source1.batchDurationMillis = 2000
tier1.sources.source1.kafka.bootstrap.servers = localhost:9092
tier1.sources.source1.kafka.topics = test1, test2
tier1.sources.source1.kafka.consumer.group.id = custom.g.id
Example for topic subscription by regex

tier1.sources.source1.type = org.apache.flume.source.kafka.KafkaSource
tier1.sources.source1.channels = channel1
tier1.sources.source1.kafka.bootstrap.servers = localhost:9092
tier1.sources.source1.kafka.topics.regex = ^topic[0-9]$
# the default kafka.consumer.group.id=flume is used

发送地址：
Flume Sinks

HDFS Sink
Example for agent named a1:

a1.channels = c1
a1.sinks = k1
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/%S
a1.sinks.k1.hdfs.filePrefix = events-
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute


The above configuration will round down the timestamp to the last 10th minute. For example, an event 
with timestamp 11:54:34 AM, June 12, 2012 will cause the hdfs path to become /flume/events/2012-06-12/1150/00.

Kafka Sink
Hive Sink
HBase Sink
File Sink

Flume Channels
Channels are the repositories where the events are staged on a agent. Source adds the events and Sink removes it.

例子
下载flume bin文件
bin/flume-ng agent --conf conf --conf-file conf/log.conf --name a1 -Dflume.root.logger=INFO,console
bin/flume-ng agent -c conf -f conf/log.conf -n a1 -Dflume.root.logger=INFO,console

# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1


$ nc localhost 44444
Trying 127.0.0.1...
Connected to localhost.localdomain (127.0.0.1).
Escape character is '^]'.
Hello world! <ENTER>
OK

2020-03-05 20:47:40,831 (SinkRunner-PollingRunner-DefaultSinkProcessor) 
[INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] 
Event: { headers:{} body: 68 65 6C 6C 6F                                  hello }

案例
监控本地log文件变化　上传至hdfs

1.
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /tmp/log/


hdfs dfs -put localfile hdfsfile
hdfs dfs -ls /
hdfs文件有sticky bit机制，只有拥有者才能删除．

案例
监控本地目录有新文件就上传
a1.sources.src-1.type = spooldir
a1.sources.src-1.channels = ch-1
a1.sources.src-1.spoolDir = /var/log/apache/flumeSpool


监控本地多个目录多个文件　上传到hdfs
!!Taildir Source + file channel + ganglia监控　保证不丢数据
flume单点故障
保证数据不丢　　
1、断点续传

注意 ： flume1.7之后有，1.7之前就需要自己写jar包上传上去使用

#source的配置
# source类型
a1.sources.r1.type = TAILDIR
# 如果flume故障，这个文件用来记录断点位置   这个文件是  自己生成的，不用提前建好，要不会报错
a1.sources.r1.positionFile = /home/hadoop/data/bd/taildir_position.json
# 监控的目录
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1=/home/hadoop/data/bd/.*log
a1.sources.r1.fileHeader = true


    Flume使用两个独立的事务分别负责从soucrce到channel，以及从channel到sink的事件传递。

put事物　　put list
take事物　 take list
比如：spooling directory source 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到channel且提交成功，
那么source就将该文件标记为完成。同理，事务以类似的方式处理从channel到sink的传递过程，如果因为某种原因使得事件无法记录，
那么事务将会回滚。且所有的事件都会保持到channel中，等待重新传递。

https://www.cnblogs.com/zhqin/p/12231632.html


一般生产过程中，都是使用 TailDir source 和 HDFS sink，所以数据会重复但是不会丢失。
channel配置说明

    MemoryChannel可以实现高速的吞吐， 但是无法保证数据完整性。
    MemoryRecoverChannel在官方文档的建议上已经建义使用FileChannel来替换。FileChannel保证数据的完整性与一致性。
    在具体配置不现的FileChannel时，建议FileChannel设置的目录和程序日志文件保存的目录设成不同的磁盘，以便提高效率。

1、背景
将data路径下所有日志文件通过Flume采集到HDFS上
五分钟一个目录，一分钟形成一个文件
2、技术选型
flume中有三种可监控文件或目录的source，分别问exec、spooldir、taildir
exec：可通过tail -f命令去tail住一个文件，然后实时同步日志到sink
spooldir：可监听一个目录，同步目录中的新文件到sink,被同步完的文件可被立即删除或被打上标记。适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步。
taildir：可实时监控一批文件，并记录每个文件最新消费位置，agent进程重启后不会有重复消费的问题。
故本次选择 taildir - file - HDFS

3、配置agent

vi taildir-file-hdfs.conf 
#agent_name
a1.sources=r1
a1.sinks=k1
a1.channels=c1

#source的配置
# source类型
a1.sources.r1.type = TAILDIR
# 元数据位置
a1.sources.r1.positionFile = /home/hadoop/data/bd/taildir_position.json
# 监控的目录
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1=/home/hadoop/data/bd/.*log
a1.sources.r1.fileHeader = true
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = timestamp

#sink的配置
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hadoop001:9000/offline/%Y%m%d/%H%M
a1.sinks.k1.hdfs.useLocalTimeStamp = true
a1.sinks.k1.hdfs.filePrefix = bd
a1.sinks.k1.hdfs.fileSuffix = .log
a1.sinks.k1.hdfs.rollSize =67108864
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.rollInterval = 60
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 5
a1.sinks.k1.hdfs.roundUnit = minute
a1.sinks.k1.hdfs.minBlockReplicas = 1
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.fileType=DataStream

#channel的配置
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /home/hadoop/data/checkpoint
a1.channels.c1.dataDirs = /home/hadoop/data
a1.channels.c1.capacity = 10000000
a1.channels.c1.transactionCapacity = 5000

#用channel链接source和sink
a1.sources.r1.channels = c1
a1.sinks.k1.channel =c1


4、启动flume
./flume-ng agent \
--name a1 \
--conf $FLUME_HOME/conf \
--conf-file /home/hadoop/script/flume/taildir-file-hdfs.conf \
-Dflume.root.logger=INFO,console

5、模拟业务数据
编写shell脚本
vi 1.sh
#!/bin/bash
cat /home/hadoop/data/bd/1.log >> /home/hadoop/data/bd/bd.log
cat /home/hadoop/data/bd/2.log >> /home/hadoop/data/bd/bd.log
cat /home/hadoop/data/bd.log >> /home/hadoop/data/bd/bd.log
cat /home/hadoop/data/bd.log >> /home/hadoop/data/bd/bd1.log
cat /home/hadoop/data/bd/1.log >> /home/hadoop/data/bd/bd1.log
cat /home/hadoop/data/bd/2.log >> /home/hadoop/data/bd/bd1.log
cat /home/hadoop/data/bd/1.log >> /home/hadoop/data/bd/bd2.log
cat /home/hadoop/data/bd/2.log >> /home/hadoop/data/bd/bd2.log

编辑crontab，添加每分钟执行1.sh
[hadoop@hadoop001 data]$ chmod +x 1.sh
[hadoop@hadoop001 data]$ crontab -e
* * * * * sh /home/hadoop/data/1.sh 

6、5分钟后查看HDFS的webui
文件夹目录


https://blog.csdn.net/weixin_38750084/article/details/82861555

